**一、实验目的**
（1）掌握利用大语言模型（LLMs）进行文本增强的基本操作。

（2）通过实验比较不同大语言模型和提示模板的生成效果，了解提示工程对模型性能的影响。

（3）学习并复现文本增强后的实验代码，验证增强文本对模型性能的提升作用。

（4）培养从实验现象中总结规律和结论的能力。

  

**二、实验原理**

  

1. **文本增强**：利用大语言模型对原始文本进行改写或扩展，以丰富训练数据的多样性。增强的文本可以提高模型的泛化能力。

2. **提示工程（Prompt Engineering）**：提示模板的设计对大语言模型的输出质量有重要影响。通过调整提示语，可以优化生成的文本质量。

3. **实验对比**：通过对比原始文本和增强文本训练的模型性能，验证文本增强的实际效果。

4. **RSTPReid数据集**：该数据集主要用于跨摄像头人员再识别任务，通过文本描述与图像匹配完成检索。

  

**三、实验步骤**

  

**3.1 文本增强实验**

  

1. **数据集准备**：打开所提供的数据集中的annos文件夹，随机选取若干条文本作为实验样本。

2. **选择大语言模型**：任选学校的“码上平台”、百度的“文心一言”、阿里的“千问”或OpenAI的GPT-4等模型进行实验。

3. **提示模板设计**：

• 使用论文中提供的提示模板：Rewrite this image caption.

• 自行尝试其他提示模板，如“Generate a detailed version of this sentence.”、“Paraphrase this description for better clarity.”等。

4. **文本增强操作**：

• 对选定的文本进行改写，记录不同模型和提示模板的生成效果。

• 尝试生成多条增强文本，观察生成质量、流畅性和语义一致性，进行优劣分析。

  

**3.2 代码复现实验**

  

1. **下载实验代码和数据集**：获取提供的代码和RSTPReid数据集，解压到指定目录。

2. **环境准备**：

• 在虚拟环境RSTT中安装requirements.txt中所需的依赖。如果报错缺少某些包，可以直接用命令pip install 包名补全。

• 将提供的nltk包放置于环境目录下（例如：C:/Users/dell/anaconda3/envs/RSTT），以避免运行时报错。

3. **配置文件修改**：

• 打开config/config_test.yaml文件，将文件中标红的路径替换为本地文件的绝对路径。
![[config1.png]]
![[config2.png]]
![[config3.png]]




  

**四、实验结果与分析**

• 通过命令行运行：python test.py，或直接在Pycharm中运行test.py文件，推理结果。

• 比较使用原始文本训练模型的推理结果和使用增强文本训练模型的推理结果，记录其性能指标和变化趋势。
  ![[eval.jpg]]

1. **文本增强实验结果**：

• 不同大语言模型生成的文本在流畅性和细节丰富度上有所差异。

• 提示模板的选择对模型输出质量影响显著，结构明确的提示语更容易引导模型生成高质量内容。

• 生成多条文本后发现，多样性和一致性难以兼顾，需要根据应用场景选择合适的增强策略。

2. **代码复现实验结果**：

• 使用原始文本训练的模型推理结果表现出一定的检索能力，但在复杂场景下存在细节缺失的问题。

• 增强文本训练的模型推理结果更加精准，对跨摄像头的复杂样本具有更高的区分能力。

• 对比指标显示，增强文本训练的模型在检索准确率和召回率方面均有显著提升。

  

**五、实验心得体会**

  

1. 本实验加深了对大语言模型在文本增强任务中的实际应用理解，提示工程对生成质量有明显影响，值得深入研究。

2. 通过比较原始文本与增强文本训练的模型性能，验证了数据增强对提升模型泛化能力的有效性。

3. 在代码复现中，配置环境和路径时要特别注意细节，否则可能导致实验中断。

4. 建议后续实验可以探索更多提示模板设计策略，进一步优化增强文本的生成效果。同时，也可尝试结合自动化评估方法，提高对生成质量的分析效率。